{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDx4QHx2Tc1J0xuR0Ozgl0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farhanwadia/nlp_g14/blob/branch_Dhairya/MIE1624_Course_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning current Working Directory\n",
        "\n",
        "Run only if using Colab"
      ],
      "metadata": {
        "id": "iRMJalDe9im3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6orfPz68l8B"
      },
      "outputs": [],
      "source": [
        "# The code below deletes the current runtime of this notebook, \n",
        "# so the following code cells will not be compiled even if you do \"Runtime>Run all\"\n",
        "# Alternate suggestion: Use \"Runtime>Run after\"\n",
        "# The purpose of this implementation is to have a fresh clone of our github repo\n",
        "# everytime we run the notebook as our current working directory\n",
        "\n",
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up the Environment"
      ],
      "metadata": {
        "id": "fHaJytAt9g_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cloning our Project Repo\n",
        "! git clone --single-branch --branch branch_Dhairya https://github.com/farhanwadia/nlp_g14.git"
      ],
      "metadata": {
        "id": "-wV1DgTR9z6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# changing the current directory to our Project Repo \"nlp_g14\", if not already done\n",
        "%cd nlp_g14"
      ],
      "metadata": {
        "id": "QaVvprzu96Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# changing the current directory to our Project Repo \"nlp_g14\", if not already done\n",
        "%cd nlp_g14"
      ],
      "metadata": {
        "id": "t8o2wE9r96lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing a Python Module \"langdetect\" which is used for detecting the language of a string variable\n",
        "# We'll be using this to filter out tweets and post which are not in english\n",
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "xIMM4CFR97Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Data"
      ],
      "metadata": {
        "id": "Y1RbihTA-dYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing basic python modules that we most probably will be using in the notebook\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.simplefilter('ignore') # ignores warnings\n",
        "\n",
        "# to make sure our plots are inline with the code cells\n",
        "%matplotlib inline \n",
        "\n",
        "np.random.seed(0) # to have the same results every time"
      ],
      "metadata": {
        "id": "Dc7N4peW-cha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the twitter training data provided to us in \"sentiment_analysis.csv\" \n",
        "# file as a pandas dataframe\n",
        "df = pd.read_csv('sentiment_analysis.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "VFyDZT-Y-nUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we do not need the \"ID\" column for our sentiment analysis so let's drop that\n",
        "df.drop(columns = ['ID'], inplace = True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9vnYhxTG-ptr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping dupicate tweets/posts (observe the difference in the shape of the dataframe after running this)\n",
        "df.drop_duplicates(subset='text',inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "c9Io2K5E-r3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating a column which indicates the language of the tweet/post\n",
        "from langdetect import detect, DetectorFactory\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "def detect_language(tweet):\n",
        "  try:\n",
        "    return detect(tweet)\n",
        "  except:\n",
        "    return None\n",
        "\n",
        "df['language'] = df['text'].apply(detect_language)\n",
        "df"
      ],
      "metadata": {
        "id": "_YJ9bP6N-uBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering out tweets/posts that are not in english (observe the difference in the shape of the dataframe after running this)\n",
        "df = df[df['language'] == 'en']\n",
        "df"
      ],
      "metadata": {
        "id": "SqYTEmG1_MC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a checkpoint since cleaning the dataframe till this cell takes long in every run\n",
        "df.to_csv('sentiment_analysis(eng_filtered).csv')"
      ],
      "metadata": {
        "id": "Wy7pYZEu_WT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checkpoint Reload"
      ],
      "metadata": {
        "id": "8eQc0fyN_sTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing basic python modules that we most probably will be using in the notebook\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.simplefilter('ignore') # ignores warnings\n",
        "\n",
        "# to make sure our plots are inline with the code cells\n",
        "%matplotlib inline \n",
        "\n",
        "np.random.seed(0) # to have the same results every time\n",
        "df = pd.read_csv('sentiment_analysis(eng_filtered).csv')\n",
        "df"
      ],
      "metadata": {
        "id": "i__DDn2p_t4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the distribution of our classes ({0: 'Negative}, {1: 'Positive'})\n",
        "fig, ax = plt.subplots(figsize = (10, 8))\n",
        "sns.countplot(x = df['label'])\n",
        "x_labels = ['Negative', 'Positive']\n",
        "ax.set_xticklabels(x_labels)\n",
        "plt.xlabel('Sentiments')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "drXmsIIF_y7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As of now our data set is unbalanced with more positive tweets than negative tweets. Moreover the sheer amount of tweets we have is too much (~ 500,000), training models with this much data will be very slow. The following cell selects equal number (5000) of random positive and negative tweets to form a balanced dataset of 10,000 tweets"
      ],
      "metadata": {
        "id": "hDdpIrMV_43_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "negative_idx = np.random.choice(df[df['label']==0].index, size = (5000)) #randomly selecting the index 5000 negative tweets/posts\n",
        "positive_idx = np.random.choice(df[df['label']==1].index, size = (5000)) #randomly selecting the index 5000 positive tweets/posts\n",
        "idx = np.concatenate((negative_idx, positive_idx)) # concatenating the indexes, we don't need to shuffle the indexes since sci-kit learn's train_test_split takes care of it\n",
        "ttr_data = df.filter(items = idx, axis = 0).reset_index(drop = True).drop(df.columns[0], axis = 1) # filtering out the indexes not present in idx\n",
        "ttr_data"
      ],
      "metadata": {
        "id": "rRFfnbR-_5x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting Training and Testing Data"
      ],
      "metadata": {
        "id": "wguDA4iJApsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "features = ttr_data.drop(columns = ['label'])\n",
        "target = ttr_data['label']\n",
        "\n",
        "# our splitting strategy would be 80%-20%, with splits stratified on target classes so that both splits will be balanced\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.20, stratify = target, random_state = 4)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.50, stratify = y_test, random_state = 4)"
      ],
      "metadata": {
        "id": "unC0hsX-AqJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check if both splits are balanced or not\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(nrows = 1, ncols = 3, sharey = True, figsize = (20,8))\n",
        "x_labels = ['Negative', 'Positive']\n",
        "sns.countplot(x = y_train, ax = ax1)\n",
        "ax1.set_title('Training Set')\n",
        "ax1.set_xticklabels(x_labels)\n",
        "ax1.set_xlabel('Sentiments')\n",
        "sns.countplot(x = y_test, ax = ax2)\n",
        "ax2.set_title('Test Set')\n",
        "ax2.set_xticklabels(x_labels)\n",
        "ax2.set_xlabel('Sentiments')\n",
        "sns.countplot(x = y_val, ax = ax3)\n",
        "ax2.set_title('Validation Set')\n",
        "ax2.set_xticklabels(x_labels)\n",
        "ax2.set_xlabel('Sentiments')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_00SmrEHAmVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Pre-Processing"
      ],
      "metadata": {
        "id": "_kv_FW0UAxrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required NLTK modules for text processsing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# defining a preprocessing function to be later used in conjunction with the Tfidf Vectorizer from Sci-kit Learn\n",
        "def PreProcessor(text):\n",
        "  lemmatizer = WordNetLemmatizer() \n",
        "  stopword = stopwords.words('english')\n",
        "  text = BeautifulSoup(text, 'html.parser').get_text() #removing html tags\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "  text = re.sub(' +', ' ', text)\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  lower = [word.lower() for word in tokens]\n",
        "  no_stopwords = [word for word in lower if word not in stopword]\n",
        "  no_alpha = [word for word in no_stopwords if word.isalpha()]\n",
        "  lemm_text = [lemmatizer.lemmatize(word) for word in no_alpha]\n",
        "  normalized_text = lemm_text\n",
        "  return normalized_text"
      ],
      "metadata": {
        "id": "G0T_EA0lAykY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Clouds"
      ],
      "metadata": {
        "id": "S2_JnFaaBQF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's checkput the word cloud of the tweets we have\n",
        "from wordcloud import WordCloud\n",
        "ttr_data['pre-processed'] = ttr_data['text'].map(PreProcessor)\n",
        "ttr_data['pre-processed text'] = ttr_data['pre-processed'].apply(lambda x: \" \".join([str(word) for word in x]))\n",
        "word_cloud = \" \".join(ttr_data['pre-processed text'].values)\n",
        "\n",
        "plt.figure(figsize=(16,13))\n",
        "wc = WordCloud(background_color=\"white\", max_words=100, max_font_size=50)\n",
        "wc.generate(word_cloud)\n",
        "plt.title(\"Most common words\", fontsize=20)\n",
        "plt.imshow(wc.recolor(colormap='Pastel2', random_state=4), alpha=0.98)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "FzN-XHc0BLtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check the word cloud of negative tweets\n",
        "word_cloud = \" \".join(ttr_data[ttr_data['label'] == 0]['pre-processed text'].values)\n",
        "\n",
        "plt.figure(figsize=(16,13))\n",
        "wc = WordCloud(background_color=\"white\", max_words=100, max_font_size=50)\n",
        "wc.generate(word_cloud)\n",
        "plt.title(\"Most common words in negative tweets\", fontsize=20)\n",
        "plt.imshow(wc.recolor(colormap='Pastel2', random_state=4), alpha=0.98)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "IFsjBBdLBTzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check the word cloud of positive tweets\n",
        "word_cloud = \" \".join(ttr_data[ttr_data['label'] == 1]['pre-processed text'].values)\n",
        "\n",
        "plt.figure(figsize=(16,13))\n",
        "wc = WordCloud(background_color=\"white\", max_words=100, max_font_size=50)\n",
        "wc.generate(word_cloud)\n",
        "plt.title(\"Most common words in Positive tweets\", fontsize=20)\n",
        "plt.imshow(wc.recolor(colormap='Pastel2', random_state=4), alpha=0.98)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "VXykRT0SBXeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5sMW8jRmBa5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF Feature Extraction"
      ],
      "metadata": {
        "id": "4Heytpx0BeQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# defining a TF-IDF vectorizing function which return the extracted TF-IDF feature in a dataframe\n",
        "def Vectorize(data, Vectorizer):\n",
        "  X = Vectorizer.transform(data)\n",
        "  words = Vectorizer.get_feature_names_out()\n",
        "  X = pd.DataFrame(X.toarray())\n",
        "  X.columns = words\n",
        "  return X"
      ],
      "metadata": {
        "id": "UsWKSBbyBhdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Vectorizer = TfidfVectorizer(analyzer = PreProcessor) # PreProcessor function we defined earlier used as an override to TfidfVectorizer's default pre-processing function\n",
        "Fitted_Vectorizer = Vectorizer.fit(X_train['text'])\n",
        "X_train = Vectorize(X_train['text'], Fitted_Vectorizer) # transforming X_train accordingly"
      ],
      "metadata": {
        "id": "6mEJ1QBhB_x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after observing the resulting X_train features, it was found that it still has \n",
        "# words from other languages so this cell attempts to drop those features\n",
        "\n",
        "from langdetect import detect\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "\n",
        "def detect_language(word):\n",
        "  try:\n",
        "    return detect(word)\n",
        "  except LangDetectException:\n",
        "    return None\n",
        "\n",
        "english_words = [word for word in X_train.columns if detect_language(word) == 'en']"
      ],
      "metadata": {
        "id": "C1Sj6Z8XCeJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(english_words)"
      ],
      "metadata": {
        "id": "27bkMN7ZEcmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train[english_words]"
      ],
      "metadata": {
        "id": "Om7UBCa1Efoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train # observe the difference in X_train.shape"
      ],
      "metadata": {
        "id": "IZMZxl9_Eldq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the same pre-processing to Validation Set\n",
        "X_val = Vectorize(X_val['text'], Fitted_Vectorizer)"
      ],
      "metadata": {
        "id": "lm7Vi1LzEvEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val = X_val[english_words]"
      ],
      "metadata": {
        "id": "J2jDVmgpE6kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val"
      ],
      "metadata": {
        "id": "WP3jNeizE9iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search for an Optimum Model\n",
        "\n",
        "Since we have a very large number of features, we are going to apply Principal Component Analysis as the first step of our pipeline and different classifiers as the second step.\n",
        "\n",
        "The following cell defines different Sci-Kit Learn Classifier models and their own parameter search space to be used in GridSearchCV to find the optimum hyperparameter combination for the classifier based on the f1_score of validation sets in a 5-fold Cross Validation Strategy\n",
        "\n",
        "At the end a function is defined which helps to visualize the results of our hyparameter search"
      ],
      "metadata": {
        "id": "tx6Lc7X0FBLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "pca = PCA()\n",
        "\n",
        "kNN = KNeighborsClassifier()\n",
        "kNN_param_grid = {'pca__n_components': [500, 1500],\n",
        "                  'kNN__n_neighbors': [3, 5, 7], \n",
        "                  'kNN__p': [1, 2]}\n",
        "\n",
        "lr = LogisticRegression(random_state = 4, max_iter = 1000)\n",
        "lr_param_grid = {'pca__n_components': [500, 1500],\n",
        "                 'lr__C': [0.1, 0.5, 1, 5], \n",
        "                 'lr__solver': ['newton-cg','lbfgs','liblinear','sag']}\n",
        "\n",
        "svc = SVC(random_state = 4)\n",
        "svc_param_grid = {'pca__n_components': [500, 1500],\n",
        "                  'svc__C': [0.1, 0.5, 1, 5], \n",
        "                  'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'], \n",
        "                  'svc__degree': [2,3,4]}\n",
        "\n",
        "xgb = XGBClassifier(random_state = 4)\n",
        "xgb_param_grid = {'pca__n_components': [500, 1500], \n",
        "                  'xgb__max_depth': [10, 20, 50],\n",
        "                  'xgb__n_estimators': [50, 100]}\n",
        "\n",
        "dtr = DecisionTreeClassifier(random_state = 4)\n",
        "dtr_param_grid = {'pca__n_components': [500, 1500],\n",
        "                  'dtr__criterion': ['gini', 'entropy', 'log_loss'], \n",
        "                  'dtr__max_depth': [10, 20, 50]}\n",
        "\n",
        "rf = RandomForestClassifier(random_state = 4)\n",
        "rf_param_grid = {'pca__n_components': [500, 1500],\n",
        "                 'rf__n_estimators': [50, 100],\n",
        "                 'rf__criterion': ['gini', 'entropy', 'log_loss'],\n",
        "                 'rf__max_depth': [10, 20, 50]}\n",
        "\n",
        "mlp = MLPClassifier(random_state = 4, max_iter = 1000)\n",
        "mlp_param_grid = {'pca__n_components': [500, 1500],\n",
        "                  'mlp__hidden_layer_sizes': [[100, 50], [50, 50]],\n",
        "                  'mlp__activation': ['logistic', 'tanh', 'relu']}\n",
        "\n",
        "\n",
        "models_dict = {'kNN': (kNN, kNN_param_grid),\n",
        "               'lr': (lr, lr_param_grid),\n",
        "               'svc': (svc, svc_param_grid),\n",
        "               'xgb': (xgb, xgb_param_grid),\n",
        "               'dtr': (dtr, dtr_param_grid),\n",
        "               'rf': (rf, rf_param_grid),\n",
        "               'mlp': (mlp, mlp_param_grid)}\n",
        "\n",
        "def print_results(results):\n",
        "    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n",
        "\n",
        "    means = results.cv_results_['mean_test_score']\n",
        "    stds = results.cv_results_['std_test_score']\n",
        "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
        "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))\n",
        "\n",
        "    best_model = results.best_estimator_\n",
        "    best_model.fit(X_train, y_train.values.ravel())\n",
        "    y_train_pred = best_model.predict(X_train)\n",
        "    y_val_pred = best_model.predict(X_val)   \n",
        "    print(\"model training score: %.3f\" % f1_score(y_train.values, y_train_pred))\n",
        "    print(\"model vaidation score: %.3f\" % f1_score(y_val.values, y_val_pred))"
      ],
      "metadata": {
        "id": "o5bdIJK0E-Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cells show the hyperparameter search results for each of the classifiers defined, (a for loop could have been written for the following cells but we didn't have the resources to run the notebook for that long so we have separately searched the optimum hyperparameter combination for each classifer)"
      ],
      "metadata": {
        "id": "puTqgR9CG6ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# kNN Classifier\n",
        "pipe = Pipeline(steps=[('pca', pca),\n",
        "                       ('kNN', kNN)])\n",
        "gridsearch = GridSearchCV(pipe, kNN_param_grid, scoring = 'f1')\n",
        "gridsearch.fit(X_train,y_train.values.ravel())\n",
        "print_results(gridsearch)"
      ],
      "metadata": {
        "id": "Gyf_l30iG0_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "pipe = Pipeline(steps=[('pca', pca),\n",
        "                       ('lr', lr)])\n",
        "gridsearch = GridSearchCV(pipe, lr_param_grid, scoring = 'f1')\n",
        "gridsearch.fit(X_train,y_train.values.ravel())\n",
        "print_results(gridsearch)"
      ],
      "metadata": {
        "id": "WwFISGGuHY6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Support Vector Machines\n",
        "pipe = Pipeline(steps=[('pca', pca),\n",
        "                       ('svc', svc)])\n",
        "gridsearch = GridSearchCV(pipe, svc_param_grid, scoring = 'f1')\n",
        "gridsearch.fit(X_train,y_train.values.ravel())\n",
        "print_results(gridsearch)"
      ],
      "metadata": {
        "id": "j-QwFfLIHjmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost Classifer\n",
        "pipe = Pipeline(steps=[('pca', pca),\n",
        "                       ('xgb', xgb)])\n",
        "gridsearch = GridSearchCV(pipe, xgb_param_grid, scoring = 'f1')\n",
        "gridsearch.fit(X_train,y_train.values.ravel())\n",
        "print_results(gridsearch)"
      ],
      "metadata": {
        "id": "P1-CbM3UHrkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Classifier\n",
        "pipe = Pipeline(steps=[('pca', pca),\n",
        "                       ('dtr', dtr)])\n",
        "gridsearch = GridSearchCV(pipe, dtr_param_grid, scoring = 'f1')\n",
        "gridsearch.fit(X_train,y_train.values.ravel())\n",
        "print_results(gridsearch)"
      ],
      "metadata": {
        "id": "Jv9ceSJ_Hxs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forests Classifier\n",
        "pipe = Pipeline(steps=[('pca', pca),\n",
        "                       ('rf', rf)])\n",
        "gridsearch = GridSearchCV(pipe, rf_param_grid, scoring = 'f1')\n",
        "gridsearch.fit(X_train,y_train.values.ravel())\n",
        "print_results(gridsearch)"
      ],
      "metadata": {
        "id": "zP-Li7pcH-kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Networks (Multi Layer Perceptron Classifier)\n",
        "pipe = Pipeline(steps=[('pca', pca),\n",
        "                       ('mlp', mlp)])\n",
        "gridsearch = GridSearchCV(pipe, mlp_param_grid, scoring = 'f1')\n",
        "gridsearch.fit(X_train,y_train.values.ravel())\n",
        "print_results(gridsearch)"
      ],
      "metadata": {
        "id": "FIDqTn2FIEFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimum Model"
      ],
      "metadata": {
        "id": "4MmlM5WfISuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the same pre-processing to Validation Set\n",
        "X_test = Vectorize(X_test['text'], Fitted_Vectorizer)"
      ],
      "metadata": {
        "id": "qu8I_Hu9KjlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X_test[english_words]"
      ],
      "metadata": {
        "id": "ya_3nwpsKsxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According the f1 scores of training and validation sets our best model is the MLPClassifier model (Neural Networks) with parameter comnbination shown in the folllowing code cell"
      ],
      "metadata": {
        "id": "65sLfbUAKit-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Model Pipeline\n",
        "best_model = Pipeline(steps=[('pca', PCA(n_components = 1500)),\n",
        "                             ('mlp', MLPClassifier(random_state = 4, \n",
        "                                                   max_iter = 1000, \n",
        "                                                   activation = 'relu', \n",
        "                                                   hidden_layer_sizes = [100, 50]))])\n",
        "best_model.fit(X_train, y_train.values.ravel())"
      ],
      "metadata": {
        "id": "IvZfEECcIRWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = best_model.predict(X_train)\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Training Set F1 Score: \", f1_score(y_train.values, y_train_pred))\n",
        "print(\"Validation Set F1 Score: \", f1_score(y_val.values, y_val_pred))\n",
        "print(\"Test Set F1 Score: \", f1_score(y_test.values, y_test_pred))"
      ],
      "metadata": {
        "id": "PHv91m08KZF6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}